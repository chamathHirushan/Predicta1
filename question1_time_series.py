# -*- coding: utf-8 -*-
"""camat1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QtQWxtmFLLdErCLHGC4W7iUmg4Bhhx4p
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error
from sklearn.compose import ColumnTransformer
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

his_df = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/historical_weather.csv')

his_df.tail(10)

plt.figure(figsize = (10,10))
sns.heatmap(his_df.sample(n = 10000, random_state = 1).isnull(), cmap  = 'viridis', cbar = False, yticklabels = False,
            # xticklabels= False
            )
plt.show()

column_names=['avg_temp_c','min_temp_c','max_temp_c','precipitation_mm','snow_depth_mm','avg_wind_dir_deg','avg_wind_speed_kmh']
for column_name in column_names:
    nan_percentage = his_df[column_name].isna().mean() * 100
    print(f"The percentage of NaN values in '{column_name}' is {nan_percentage:.2f}%")

min_ = his_df['snow_depth_mm'].min()
print(f"The minimum snow_depth_mm is {min_}")

max_ = his_df['snow_depth_mm'].max()
print(f"The maximum snow_depth_mm is {max_}")

zero_count = (his_df['snow_depth_mm'] == 0).sum()
print(f"The number of zero values in snow_depth_mmis {zero_count}")

total_count = his_df['snow_depth_mm'] .count()
# Compute the percentage
zero_percentage = (zero_count / total_count) * 100

print(f"The percentage of zero values in 'age' is {zero_percentage:.2f}%")

#his_df['snow_depth_mm'] = his_df['snow_depth_mm'].fillna(0)

"""**snow_depth_mm** should be removed and others should be checked further"""

his_df = his_df.drop(columns=['snow_depth_mm'])

his_df.dtypes

his_df['date'] = pd.to_datetime(his_df['date'])

# his_df['year'] = his_df['date'].dt.year
# his_df['month'] = his_df['date'].dt.month
# his_df['day'] = his_df['date'].dt.day

# his_df=his_df.drop(columns=['date'])

his_df.columns

his_df['city_id'].unique()

metric_df=his_df.drop(columns=['city_id', 'date'])

correlation_matrix = metric_df.corr()
plt.figure(figsize=(7,7))
sns.heatmap(correlation_matrix, annot = True, cmap='coolwarm',fmt=".2f")
plt.title("correlation Heatmap")
plt.show()

his_df=his_df.drop(columns=['min_temp_c','max_temp_c'])

his_df.columns

# Select features for plotting
features = ['avg_temp_c', 'precipitation_mm', 'avg_wind_dir_deg', 'avg_wind_speed_kmh']

# Plot the distribution of each variable over time
plt.figure(figsize=(15, 10))
for i, feature in enumerate(features, 1):
    plt.subplot(len(features), 1, i)
    sns.lineplot(data=his_df, x='date', y=feature)
    plt.title(f'{feature} over Time')
    plt.xlabel('Date')
    plt.ylabel(feature)
    plt.tight_layout()

plt.show()

"""Note - Average temp shows a clear seasonal variation than others, therefore its better to use that directly for the prediction"""

import warnings

# Suppress all warnings
warnings.filterwarnings('ignore')

# Unique cities
cities = his_df['city_id'].unique()

# Calculate the number of rows for the subplot grid
n_cities = len(cities)
n_rows = (n_cities + 2) // 3  # 3 plots per row

# Plot avg_temp_c over time for each city
plt.figure(figsize=(18, n_rows * 5))
for i, city in enumerate(cities, 1):
    plt.subplot(n_rows, 3, i)
    city_data = his_df[his_df['city_id'] == city]
    sns.lineplot(data=city_data, x='date', y='avg_temp_c')
    plt.title(f'Average Temperature Over Time for {city}')
    plt.xlabel('Date')
    plt.ylabel('avg_temp_c')

plt.tight_layout()
plt.show()

"""note- therefore we need seasonal models, rather than simpler nes like moving averages"""

his_df.columns

df=his_df.drop(columns=['precipitation_mm','avg_wind_dir_deg','avg_wind_speed_kmh'])

df.tail(5)

train = df[(df['date'] >= '2014-01-01') & (df['date'] < '2018-01-01')]
val = df[(df['date'] >= '2018-01-01') & (df['date'] < '2018-01-08')]

from prophet import Prophet
from sklearn.metrics import mean_squared_error

city = 'C001' # sample city that concidered
city_data = train[train['city_id'] == city][['date', 'avg_temp_c']]
city_val = val[val['city_id'] == city][['date', 'avg_temp_c']]

city_val.tail(7)

# Rename columns for Prophet
city_data.rename(columns={'date': 'ds', 'avg_temp_c': 'y'}, inplace=True)

# Initialize Prophet model
model = Prophet()

# Fit the model
model.fit(city_data)

# Make a DataFrame with future dates for the next week
future_dates = model.make_future_dataframe(periods=7)

# Make predictions
forecast = model.predict(future_dates)

# Plot the forecast
fig = model.plot(forecast)
plt.title(f'Average Temperature Forecast for {city}')
plt.xlabel('Date')
plt.ylabel('Average Temperature (C)')
plt.show()

# Display forecasted values
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)

# Assuming y_true contains the actual observed values
y_true = city_val['avg_temp_c'].values[-7:]  # Actual values for the last 7 days
y_pred = forecast['yhat'].values[-7:]  # Predicted values for the last 7 days
y_pred_low = forecast['yhat_lower'].values[-7:]
y_pred_upp = forecast['yhat_upper'].values[-7:]

# Calculate MAE
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
rmse_low = np.sqrt(mean_squared_error(y_true, y_pred_low))
rmse_upp = np.sqrt(mean_squared_error(y_true, y_pred_upp))
print("Root Mean Squared Error (RMSE) with y-hat:", rmse)
print("Root Mean Squared Error (RMSE) with y-hat-lower:", rmse_low)
print("Root Mean Squared Error (RMSE) with y-hat-upper:", rmse_upp)

"""For each city, predict the the value of first 7 days, and decide whether to choose mae, mae_lower or mae_upper for the next year predictions"""

cities = his_df['city_id'].unique()

# Create a dictionary with city IDs as keys and indices as values
city_dict_err = {city: index for index, city in enumerate(cities)}
city_dict_model = {city: index for index, city in enumerate(cities)}

# Print the dictionary
print(city_dict_err)

city_dict['C001']

err=0
count=0

for city in cities:
    city_data = train[train['city_id'] == city][['date', 'avg_temp_c']]
    city_val = val[val['city_id'] == city][['date', 'avg_temp_c']]

    # Rename columns for Prophet
    city_data.rename(columns={'date': 'ds', 'avg_temp_c': 'y'}, inplace=True)

    # Initialize Prophet model
    model = Prophet()

    # Fit the model
    model.fit(city_data)

    # Make a DataFrame with future dates for the next week
    future_dates = model.make_future_dataframe(periods=7)

    # Make predictions
    forecast = model.predict(future_dates)

    # # Plot the forecast
    # fig = model.plot(forecast)
    # plt.title(f'Average Temperature Forecast for {city}')
    # plt.xlabel('Date')
    # plt.ylabel('Average Temperature (C)')
    # plt.show()

    # Display forecasted values
    #forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)

    # Assuming y_true contains the actual observed values
    y_true = city_val['avg_temp_c'].values[-7:]  # Actual values for the last 7 days
    y_pred = forecast['yhat'].values[-7:]  # Predicted values for the last 7 days
    y_pred_low = forecast['yhat_lower'].values[-7:]
    y_pred_upp = forecast['yhat_upper'].values[-7:]

    # Calculate RMSE
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    rmse_low = np.sqrt(mean_squared_error(y_true, y_pred_low))
    rmse_upp = np.sqrt(mean_squared_error(y_true, y_pred_upp))


    min_rmse = min(rmse, rmse_low, rmse_upp)

    # Determine which variable has the lowest value
    if min_rmse == rmse:
        lowest_name = 'yhat'
    elif min_rmse == rmse_low:
        lowest_name = 'yhat_lower'
    else:
        lowest_name = 'yhat_upper'

    city_dict_err[city]=lowest_name
    city_dict_model[city]=model
    err+=min_rmse
    count+=1

avg_err = err / count
    print(avg_err)

"""now need to predict the first 7 days, using those saved model and the lowest_name s

**Predicitng part**
"""

sub_df = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/submission_key.csv')

sub_df.dtypes

sub_df['date'] = pd.to_datetime(sub_df['date'])

sub_df.dtypes

sub_df.head(5)

submission_P201 = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/sample_submission.csv')

train = df[(df['date'] >= '2014-01-01') & (df['date'] < '2019-01-01')]

new_city_dict_model = {city: index for index, city in enumerate(cities)}

for city in cities:
    city_data = train[train['city_id'] == city][['date', 'avg_temp_c']]
    #city_val = val[val['city_id'] == city][['date', 'avg_temp_c']]

    # Rename columns for Prophet
    city_data.rename(columns={'date': 'ds', 'avg_temp_c': 'y'}, inplace=True)

    # Initialize Prophet model
    model = Prophet()

    # Fit the model
    model.fit(city_data)

    # Make a DataFrame with future dates for the next week
    future_dates = model.make_future_dataframe(periods=7)

    # Make predictions
    #forecast = model.predict(future_dates)
    new_city_dict_model[city]=model

for index, row in sub_df.iterrows():
    # Access elements of each row using column names
    #print(row['city_id'])
    city_model=new_city_dict_model[row['city_id']] #select the model

    future_date = pd.DataFrame({'ds': [row['date']]})

    # Use the trained model to make a prediction for the specific date
    forecast_specific_date = city_model.predict(future_date)
    #print(forecast_specific_date)
    predicted_value = forecast_specific_date[ 'yhat'].values[0] #city_dict_err[city]].values[0]

    submission_P201.loc[index, 'avg_temp_c'] = predicted_value

    submission_id_df1 = sub_df.at[index, 'submission_ID']
    submission_id_df2 = submission_P201.at[index, 'submission_ID']

    # Compare the submission_id values
    if not submission_id_df1 == submission_id_df2:
        print("submission_id is not equal on the row with index", known_index)

submission_P201.tail(10)

submission_P201.to_csv('submission.csv', index=False)

"""**SARIMA**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Dictionary to store the models for each city
new_city_sarima_model = {}

# List of cities
cities = df['city_id'].unique()

for city in cities:
    city_data = train[train['city_id'] == city][['date', 'avg_temp_c']]

    # Prepare the data
    city_data.set_index('date', inplace=True)
    city_data.index = pd.to_datetime(city_data.index)
    city_data = city_data.asfreq('D')

    # Initialize and fit SARIMA model
    model = SARIMAX(city_data['avg_temp_c'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))
    model_fit = model.fit(disp=False)

    # Make predictions for the next week
    forecast = model_fit.get_forecast(steps=7)
    forecast_index = pd.date_range(start=city_data.index[-1] + pd.Timedelta(days=1), periods=7, freq='D')
    forecast_values = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    # Store the model
    new_city_sarima_model[city] = model_fit

    # Print the forecast
    print(f"Forecast for {city}:")
#     print(forecast_values)
#     print(f"Confidence Interval:\n{forecast_ci}")

type(forecast_values)

forecast_values

for index, row in sub_df.iterrows():
    city_id = row['city_id']
    prediction_date = pd.to_datetime(row['date'])

    # Select the SARIMA model for the specific city

    city_model = new_city_sarima_model[city_id]

    last_train_date = pd.to_datetime("2018-12-31")
    forecast_steps = (prediction_date - last_train_date).days


    forecast = city_model.get_forecast(steps=forecast_steps)
    forecast_mean = forecast.predicted_mean
    predicted_value = forecast_mean.loc[prediction_date]

    # Update the submission DataFrame with the predicted value
    submission_P201.loc[index, 'avg_temp_c'] = predicted_value

    # Compare the submission_id values between the two DataFrames
    submission_id_df1 = sub_df.at[index, 'submission_ID']
    submission_id_df2 = submission_P201.at[index, 'submission_ID']

    if submission_id_df1 != submission_id_df2:
        print("submission_id is not equal on the row with index", index)

submission_P201.tail(5)

submission_P201.to_csv('submission_P201_new.csv', index=False)