# -*- coding: utf-8 -*-
"""notebook2 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D2K34OGw3I2gOkeuoWiPFuX-7Njrq5XB
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

daily_data = pd.read_csv("/kaggle/input/predicta-1-0-predict-the-unpredictable-part-2/daily_data.csv")
submission = pd.read_csv("/kaggle/input/predicta-1-0-predict-the-unpredictable-part-2/submission.csv")

submission.head()

daily_data.head()

# Check for missing values
print(daily_data.isnull().sum())

print(daily_data.dtypes)

daily_data.describe()

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Handle missing values
# Fill missing condition_text with a placeholder
daily_data['condition_text'].fillna('Missing', inplace=True)

# Convert sunrise and sunset to datetime and extract useful features
daily_data['sunrise'] = pd.to_datetime(daily_data['sunrise'], format='%I:%M %p').dt.hour * 60 + pd.to_datetime(daily_data['sunrise'], format='%I:%M %p').dt.minute
daily_data['sunset'] = pd.to_datetime(daily_data['sunset'], format='%I:%M %p').dt.hour * 60 + pd.to_datetime(daily_data['sunset'], format='%I:%M %p').dt.minute

# Example of creating a day length feature
daily_data['day_length'] = daily_data['sunset'] - daily_data['sunrise']

daily_data.drop(columns=['sunrise', 'sunset'],axis=1, inplace = True )

daily_data.head()

import matplotlib.pyplot as plt
import seaborn as sns

features_to_check = ['temperature_celsius', 'wind_kph', 'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',
                     'cloud', 'feels_like_celsius', 'visibility_km', 'uv_index', 'gust_kph',
                     'air_quality_us-epa-index', 'day_length']

for feature in features_to_check:
    plt.figure(figsize=(8, 6))
    sns.histplot(daily_data[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()

skewed_features = ['wind_kph', 'pressure_mb', 'precip_mm', 'humidity', 'gust_kph', 'day_length']

for feature in skewed_features:
    daily_data[feature] = np.log1p(daily_data[feature])  # log1p is equivalent to log(x + 1)

daily_data.head()

# Encode the target variable (excluding 'Missing' rows)
known_conditions = daily_data[daily_data['condition_text'] != 'Missing']
le = LabelEncoder()
known_conditions['condition_text_encoded'] = le.fit_transform(known_conditions['condition_text'])

# Split data into features and target
features = known_conditions.drop(columns=['day_id', 'city_id', 'condition_text', 'condition_text_encoded'])
target = known_conditions['condition_text_encoded']

features.head()

target.head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

from xgboost import XGBClassifier

xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'XGBoost Model Accuracy: {accuracy:.2f}')

import optuna

# Define objective function for Optuna
def objective(trial):
    param = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
    }

    model = XGBClassifier(**param, random_state=42)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    return accuracy

# Optimize hyperparameters
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Train the final model with the best hyperparameters
best_params = study.best_params
print(f'Best Parameters: {best_params}')

xgb_model = XGBClassifier(**best_params, random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Tuned XGBoost Model Accuracy: {accuracy:.2f}')

"""Hyperparameter tuning"""

# from sklearn.model_selection import GridSearchCV
# import xgboost as xgb

# # Define the parameter grid
# param_grid = {
#     'n_estimators': [100, 200, 300],
#     'learning_rate': [0.01, 0.05, 0.1],
#     'max_depth': [3, 5, 7],
#     'subsample': [0.8, 0.9, 1.0],
#     'colsample_bytree': [0.8, 0.9, 1.0]
# }

# # Initialize the XGBoost model
# xgb_model = xgb.XGBClassifier(random_state=42)

# # Set up the grid search
# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)

# # Fit the grid search
# grid_search.fit(X_train, y_train)

# # Get the best parameters and score
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print(f"Best Parameters: {best_params}")
# print(f"Best Score: {best_score:.2f}")

# # Train the model with the best parameters
# xgb_model = xgb.XGBClassifier(**best_params, random_state=42)
# xgb_model.fit(X_train, y_train)

# # Make predictions
# y_pred = xgb_model.predict(X_test)

# # Evaluate the model
# accuracy = accuracy_score(y_test, y_pred)
# print(f'Tuned XGBoost Model Accuracy: {accuracy:.2f}')

# from catboost import CatBoostClassifier

# # Train a CatBoost Classifier
# catboost_model = CatBoostClassifier(iterations=100, random_state=42, verbose=0)
# catboost_model.fit(X_train, y_train)

# # Make predictions
# y_pred = catboost_model.predict(X_test)

# # Evaluate the model
# accuracy = accuracy_score(y_test, y_pred)
# print(f'CatBoost Model Accuracy: {accuracy:.2f}')

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)
print(f'Cross-validated scores: {cv_scores}')
print(f'Mean cross-validated accuracy: {np.mean(cv_scores):.2f}')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Predict missing values in the original dataset
missing_data = daily_data[daily_data['condition_text'] == 'Missing']
missing_features = missing_data.drop(columns=['day_id', 'city_id', 'condition_text'])

# Predict using the trained model
predicted_conditions = xgb_model.predict(missing_features)
missing_data['condition_text'] = le.inverse_transform(predicted_conditions)

# Combine with original dataset
daily_data.loc[daily_data['condition_text'] == 'Missing', 'condition_text'] = missing_data['condition_text']

# Create submission file
submission = daily_data[['day_id', 'condition_text']]
submission.to_csv('submission.csv', index=False)

submission.head()

